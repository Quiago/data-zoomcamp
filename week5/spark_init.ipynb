{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/09 20:39:15 WARN Utils: Your hostname, quiala-IdeaPad-3-15ITL6 resolves to a loopback address: 127.0.1.1; using 192.168.1.101 instead (on interface wlp1s0)\n",
      "25/04/09 20:39:15 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/04/09 20:39:17 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.master(\"local[*]\").appName(\"test\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-------------------+------------+------------+-------+----------------------+\n",
      "|dispatching_base_num|    pickup_datetime|   dropOff_datetime|PUlocationID|DOlocationID|SR_Flag|Affiliated_base_number|\n",
      "+--------------------+-------------------+-------------------+------------+------------+-------+----------------------+\n",
      "|              B00037|2019-02-01 00:08:44|2019-02-01 00:23:35|         264|         265|   NULL|                B00037|\n",
      "|              B00037|2019-02-01 00:27:51|2019-02-01 00:32:54|         264|         265|   NULL|                B00037|\n",
      "|              B00037|2019-02-01 00:18:30|2019-02-01 00:25:45|         264|         265|   NULL|                B00037|\n",
      "|              B00037|2019-02-01 00:43:15|2019-02-01 00:48:29|         264|         265|   NULL|                B00037|\n",
      "|              B00037|2019-02-01 00:01:45|2019-02-01 00:09:13|         264|         265|   NULL|                B00037|\n",
      "|              B00037|2019-02-01 00:03:51|2019-02-01 00:16:36|         264|         265|   NULL|                B00037|\n",
      "|              B00037|2019-02-01 00:07:04|2019-02-01 00:13:47|         264|         265|   NULL|                B00037|\n",
      "|              B00037|2019-02-01 00:02:09|2019-02-01 00:07:32|         264|         265|   NULL|                B00037|\n",
      "|              B00095|2019-02-01 00:18:58|2019-02-01 00:25:52|         264|         265|   NULL|                B00095|\n",
      "|              B00095|2019-02-01 00:59:51|2019-02-01 01:06:07|         264|         265|   NULL|                B00095|\n",
      "|              B00112|2019-02-01 00:01:28|2019-02-01 00:10:33|         264|         265|   NULL|                B00112|\n",
      "|              B00112|2019-02-01 00:25:39|2019-02-01 00:25:43|         264|         265|   NULL|                B00112|\n",
      "|              B00112|2019-02-01 00:47:31|2019-02-01 01:16:05|         264|         265|   NULL|                B00112|\n",
      "|              B00112|2019-02-01 00:00:07|2019-02-01 00:33:15|         264|         265|   NULL|                B00112|\n",
      "|              B00131|2019-02-01 00:57:35|2019-02-01 01:09:06|         264|         265|   NULL|                B00131|\n",
      "|              B00160|2019-02-01 00:32:00|2019-02-01 00:36:00|         264|         264|   NULL|                B00160|\n",
      "|              B00160|2019-02-01 00:07:00|2019-02-01 00:16:00|         264|         264|   NULL|                B00160|\n",
      "|              B00160|2019-02-01 00:46:00|2019-02-01 01:00:00|         264|         264|   NULL|                B00160|\n",
      "|              B00160|2019-02-01 00:07:00|2019-02-01 00:21:00|         264|         264|   NULL|                B00160|\n",
      "|              B00221|2019-02-01 00:05:03|2019-02-01 00:15:48|         264|         265|   NULL|                B00221|\n",
      "+--------------------+-------------------+-------------------+------------+------------+-------+----------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.option(\"header\", \"true\").csv(\"fhv_tripdata_2019-02.csv\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType([StructField('dispatching_base_num', StringType(), True), StructField('pickup_datetime', StringType(), True), StructField('dropOff_datetime', StringType(), True), StructField('PUlocationID', StringType(), True), StructField('DOlocationID', StringType(), True), StructField('SR_Flag', StringType(), True), StructField('Affiliated_base_number', StringType(), True)])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = types.StructType([\n",
    "    types.StructField('dispatching_base_num', types.StringType(), True), \n",
    "    types.StructField('pickup_datetime', types.TimestampType(), True), \n",
    "    types.StructField('dropOff_datetime', types.TimestampType(), True), \n",
    "    types.StructField('PUlocationID', types.IntegerType(), True), \n",
    "    types.StructField('DOlocationID', types.IntegerType(), True), \n",
    "    types.StructField('SR_Flag', types.IntegerType(), True), \n",
    "    types.StructField('Affiliated_base_number', types.StringType(), True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.option(\"header\", \"true\").schema(schema).csv(\"fhv_tripdata_2019-02.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(dispatching_base_num='B00037', pickup_datetime=datetime.datetime(2019, 2, 1, 0, 8, 44), dropOff_datetime=datetime.datetime(2019, 2, 1, 0, 23, 35), PUlocationID=264, DOlocationID=265, SR_Flag=None, Affiliated_base_number='B00037'),\n",
       " Row(dispatching_base_num='B00037', pickup_datetime=datetime.datetime(2019, 2, 1, 0, 27, 51), dropOff_datetime=datetime.datetime(2019, 2, 1, 0, 32, 54), PUlocationID=264, DOlocationID=265, SR_Flag=None, Affiliated_base_number='B00037'),\n",
       " Row(dispatching_base_num='B00037', pickup_datetime=datetime.datetime(2019, 2, 1, 0, 18, 30), dropOff_datetime=datetime.datetime(2019, 2, 1, 0, 25, 45), PUlocationID=264, DOlocationID=265, SR_Flag=None, Affiliated_base_number='B00037'),\n",
       " Row(dispatching_base_num='B00037', pickup_datetime=datetime.datetime(2019, 2, 1, 0, 43, 15), dropOff_datetime=datetime.datetime(2019, 2, 1, 0, 48, 29), PUlocationID=264, DOlocationID=265, SR_Flag=None, Affiliated_base_number='B00037'),\n",
       " Row(dispatching_base_num='B00037', pickup_datetime=datetime.datetime(2019, 2, 1, 0, 1, 45), dropOff_datetime=datetime.datetime(2019, 2, 1, 0, 9, 13), PUlocationID=264, DOlocationID=265, SR_Flag=None, Affiliated_base_number='B00037'),\n",
       " Row(dispatching_base_num='B00037', pickup_datetime=datetime.datetime(2019, 2, 1, 0, 3, 51), dropOff_datetime=datetime.datetime(2019, 2, 1, 0, 16, 36), PUlocationID=264, DOlocationID=265, SR_Flag=None, Affiliated_base_number='B00037'),\n",
       " Row(dispatching_base_num='B00037', pickup_datetime=datetime.datetime(2019, 2, 1, 0, 7, 4), dropOff_datetime=datetime.datetime(2019, 2, 1, 0, 13, 47), PUlocationID=264, DOlocationID=265, SR_Flag=None, Affiliated_base_number='B00037'),\n",
       " Row(dispatching_base_num='B00037', pickup_datetime=datetime.datetime(2019, 2, 1, 0, 2, 9), dropOff_datetime=datetime.datetime(2019, 2, 1, 0, 7, 32), PUlocationID=264, DOlocationID=265, SR_Flag=None, Affiliated_base_number='B00037'),\n",
       " Row(dispatching_base_num='B00095', pickup_datetime=datetime.datetime(2019, 2, 1, 0, 18, 58), dropOff_datetime=datetime.datetime(2019, 2, 1, 0, 25, 52), PUlocationID=264, DOlocationID=265, SR_Flag=None, Affiliated_base_number='B00095'),\n",
       " Row(dispatching_base_num='B00095', pickup_datetime=datetime.datetime(2019, 2, 1, 0, 59, 51), dropOff_datetime=datetime.datetime(2019, 2, 1, 1, 6, 7), PUlocationID=264, DOlocationID=265, SR_Flag=None, Affiliated_base_number='B00095')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.repartition(24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df\u001b[38;5;241m.\u001b[39mwrite\u001b[38;5;241m.\u001b[39mparquet(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfhvhv/2019/02/\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "df.write.parquet(\"fhvhv/2019/02/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/09 20:39:32 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df = spark.read.parquet(\"fhvhv/2019/02/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- dispatching_base_num: string (nullable = true)\n",
      " |-- pickup_datetime: timestamp (nullable = true)\n",
      " |-- dropOff_datetime: timestamp (nullable = true)\n",
      " |-- PUlocationID: integer (nullable = true)\n",
      " |-- DOlocationID: integer (nullable = true)\n",
      " |-- SR_Flag: integer (nullable = true)\n",
      " |-- Affiliated_base_number: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(dispatching_base_num='B01239', pickup_datetime=datetime.datetime(2019, 2, 2, 9, 32, 51), dropOff_datetime=datetime.datetime(2019, 2, 2, 9, 38, 34), PUlocationID=264, DOlocationID=265, SR_Flag=None, Affiliated_base_number='B02869'),\n",
       " Row(dispatching_base_num='B03060', pickup_datetime=datetime.datetime(2019, 2, 1, 18, 14, 17), dropOff_datetime=datetime.datetime(2019, 2, 1, 18, 38, 3), PUlocationID=264, DOlocationID=265, SR_Flag=None, Affiliated_base_number='B03060'),\n",
       " Row(dispatching_base_num='B00856', pickup_datetime=datetime.datetime(2019, 2, 4, 15, 48, 44), dropOff_datetime=datetime.datetime(2019, 2, 4, 16, 27, 43), PUlocationID=264, DOlocationID=265, SR_Flag=None, Affiliated_base_number='B02887'),\n",
       " Row(dispatching_base_num='B01469', pickup_datetime=datetime.datetime(2019, 2, 2, 2, 19, 29), dropOff_datetime=datetime.datetime(2019, 2, 2, 2, 30, 39), PUlocationID=264, DOlocationID=265, SR_Flag=None, Affiliated_base_number='B01469'),\n",
       " Row(dispatching_base_num='B01239', pickup_datetime=datetime.datetime(2019, 2, 4, 8, 49, 14), dropOff_datetime=datetime.datetime(2019, 2, 4, 8, 52, 34), PUlocationID=264, DOlocationID=265, SR_Flag=None, Affiliated_base_number='B01239'),\n",
       " Row(dispatching_base_num='B01469', pickup_datetime=datetime.datetime(2019, 2, 2, 17, 32, 24), dropOff_datetime=datetime.datetime(2019, 2, 2, 17, 43, 46), PUlocationID=264, DOlocationID=265, SR_Flag=None, Affiliated_base_number='B01469'),\n",
       " Row(dispatching_base_num='B02715', pickup_datetime=datetime.datetime(2019, 2, 4, 14, 8, 35), dropOff_datetime=datetime.datetime(2019, 2, 4, 14, 57, 53), PUlocationID=132, DOlocationID=256, SR_Flag=None, Affiliated_base_number='B02883'),\n",
       " Row(dispatching_base_num='B01445', pickup_datetime=datetime.datetime(2019, 2, 1, 6, 43), dropOff_datetime=datetime.datetime(2019, 2, 1, 7, 40), PUlocationID=265, DOlocationID=265, SR_Flag=None, Affiliated_base_number='B01445'),\n",
       " Row(dispatching_base_num='B00900', pickup_datetime=datetime.datetime(2019, 2, 1, 23, 52, 36), dropOff_datetime=datetime.datetime(2019, 2, 1, 23, 59, 27), PUlocationID=264, DOlocationID=265, SR_Flag=None, Affiliated_base_number='B00900'),\n",
       " Row(dispatching_base_num='B01061', pickup_datetime=datetime.datetime(2019, 2, 2, 6, 24, 11), dropOff_datetime=datetime.datetime(2019, 2, 2, 6, 47, 20), PUlocationID=264, DOlocationID=265, SR_Flag=None, Affiliated_base_number='B01061')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+\n",
      "|    pickup_datetime|   dropoff_datetime|\n",
      "+-------------------+-------------------+\n",
      "|2019-02-02 09:32:51|2019-02-02 09:38:34|\n",
      "|2019-02-04 08:49:14|2019-02-04 08:52:34|\n",
      "|2019-02-01 14:21:37|2019-02-01 14:26:37|\n",
      "|2019-02-04 08:43:48|2019-02-04 08:53:24|\n",
      "|2019-02-02 14:26:36|2019-02-02 14:39:40|\n",
      "|2019-02-04 14:42:40|2019-02-04 14:46:24|\n",
      "|2019-02-04 09:53:45|2019-02-04 10:02:35|\n",
      "|2019-02-03 01:25:47|2019-02-03 01:41:57|\n",
      "|2019-02-02 19:30:48|2019-02-02 19:49:14|\n",
      "|2019-02-02 10:40:50|2019-02-02 10:45:10|\n",
      "|2019-02-04 04:42:52|2019-02-04 04:53:15|\n",
      "|2019-02-01 21:23:03|2019-02-01 21:27:51|\n",
      "|2019-02-03 10:17:49|2019-02-03 10:26:31|\n",
      "|2019-02-02 20:15:43|2019-02-02 20:21:04|\n",
      "|2019-02-02 18:15:45|2019-02-02 18:36:53|\n",
      "|2019-02-01 07:34:32|2019-02-01 08:03:27|\n",
      "|2019-02-03 01:25:23|2019-02-03 01:37:01|\n",
      "|2019-02-03 06:48:18|2019-02-03 06:56:52|\n",
      "|2019-02-02 21:45:41|2019-02-02 21:52:48|\n",
      "|2019-02-01 06:36:55|2019-02-01 06:50:03|\n",
      "+-------------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('pickup_datetime', 'dropoff_datetime').filter(df.dispatching_base_num == 'B01239').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crazzy_stuff(base_num):\n",
    "    num = int(base_num[1:])\n",
    "    if num % 7 == 0:\n",
    "        return f'/s/{num:03x}'\n",
    "    elif num % 3 == 0:\n",
    "        return f'/s/{num:03x}'\n",
    "    else:\n",
    "        return f'/s/{num:03x}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "crazy_stuff_udf = F.udf(crazzy_stuff, returnType=types.StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 3:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+-----------+------------+\n",
      "|base_id|dispatching_base_num|pickup_date|dropoff_date|\n",
      "+-------+--------------------+-----------+------------+\n",
      "| /s/4d7|              B01239| 2019-02-02|  2019-02-02|\n",
      "| /s/bf4|              B03060| 2019-02-01|  2019-02-01|\n",
      "| /s/358|              B00856| 2019-02-04|  2019-02-04|\n",
      "| /s/5bd|              B01469| 2019-02-02|  2019-02-02|\n",
      "| /s/4d7|              B01239| 2019-02-04|  2019-02-04|\n",
      "| /s/5bd|              B01469| 2019-02-02|  2019-02-02|\n",
      "| /s/a9b|              B02715| 2019-02-04|  2019-02-04|\n",
      "| /s/5a5|              B01445| 2019-02-01|  2019-02-01|\n",
      "| /s/384|              B00900| 2019-02-01|  2019-02-01|\n",
      "| /s/425|              B01061| 2019-02-02|  2019-02-02|\n",
      "| /s/80a|              B02058| 2019-02-01|  2019-02-01|\n",
      "| /s/6be|              B01726| 2019-02-02|  2019-02-02|\n",
      "| /s/358|              B00856| 2019-02-03|  2019-02-03|\n",
      "| /s/b53|              B02899| 2019-02-04|  2019-02-04|\n",
      "| /s/505|              B01285| 2019-02-02|  2019-02-02|\n",
      "| /s/1ad|              B00429| 2019-02-04|  2019-02-04|\n",
      "| /s/c55|              B03157| 2019-02-01|  2019-02-01|\n",
      "| /s/59d|              B01437| 2019-02-03|  2019-02-03|\n",
      "| /s/af3|              B02803| 2019-02-02|  2019-02-02|\n",
      "| /s/523|              B01315| 2019-02-02|  2019-02-02|\n",
      "+-------+--------------------+-----------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df \\\n",
    ".withColumn('pickup_date', F.to_date(df.pickup_datetime)) \\\n",
    ".withColumn('dropoff_date', F.to_date(df.dropOff_datetime)) \\\n",
    ".withColumn('base_id', crazy_stuff_udf(df.dispatching_base_num)) \\\n",
    ".select('base_id','dispatching_base_num','pickup_date', 'dropoff_date') \\\n",
    ".show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_science",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
